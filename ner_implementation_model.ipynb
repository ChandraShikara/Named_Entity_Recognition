{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11518,
     "status": "ok",
     "timestamp": 1761813104338,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "6PTPN1LIMo8T",
    "outputId": "b61b46f7-e35b-4e35-bd6a-799eb75e648c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.12/dist-packages (0.9.11)\n",
      "Requirement already satisfied: torchcrf in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio transformers datasets seqeval numpy pandas tqdm python-crfsuite torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1761812538120,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "f0oufUAxMwlt",
    "outputId": "4f5e3407-2f42-43eb-d9d7-2b6aecfc9d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    29  100    29    0     0     80      0 --:--:-- --:--:-- --:--:--    80\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    29  100    29    0     0     84      0 --:--:-- --:--:-- --:--:--    84\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    29  100    29    0     0    101      0 --:--:-- --:--:-- --:--:--   101\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o conll2003_train.json https://huggingface.co/datasets/mesolitica/conll2003/resolve/main/conll2003_train.json\n",
    "!curl -L -o conll2003_validation.json https://huggingface.co/datasets/mesolitica/conll2003/resolve/main/conll2003_validation.json\n",
    "!curl -L -o conll2003_test.json https://huggingface.co/datasets/mesolitica/conll2003/resolve/main/conll2003_test.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15808,
     "status": "ok",
     "timestamp": 1761813120159,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "MuAb_QsJM3N7",
    "outputId": "cc3f100a-0022-4e9b-c4bf-6afa400a5ea9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7538,
     "status": "ok",
     "timestamp": 1761813127701,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "kOheNZJvNh7b",
    "outputId": "c811317d-95cb-47d8-bbc8-6d1c7dfded73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "28799729f16b44d0a2575eff257c3461",
      "62aac2d9bacb470f9d9a2e1dffb0fd54",
      "665da787759e48a5bd9b7c272af47dd3",
      "347fca5bd36c4ac3bba2f4bb67e49582",
      "1af1ce9212f64aeb9f67b6ee2f1f0073",
      "acf999db173c47beab3ca6f0da5a56ff",
      "2014e15ea1e64ea19b740b23daf2686c",
      "4bb6d8e892ee40349deb32d5b705b210",
      "e5d5147f4a9b418fa516c3f80dce43b8",
      "56c3b8d8b04d42cab0f4144ab2044116",
      "1704141d3f7c4a5081e29720b527f178"
     ]
    },
    "executionInfo": {
     "elapsed": 1711,
     "status": "ok",
     "timestamp": 1761813129416,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "vWMwFKPTOMxa",
    "outputId": "5b5d49cb-d0d5-468e-9dac-781d2f26ee2c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28799729f16b44d0a2575eff257c3461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5707,
     "status": "ok",
     "timestamp": 1761813135161,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "B5GgU6UNOc7f"
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4246,
     "status": "ok",
     "timestamp": 1761813139485,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "0Fq1bE4FO2J4",
    "outputId": "aff15d7f-8440-4b06-a277-c99d5f50f3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4948,
     "status": "ok",
     "timestamp": 1761813144435,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "MdSlyfdMPYl3",
    "outputId": "8a7be569-ed94-4bea-996c-30d3c226c201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.57.1\n",
      "datasets 4.3.0\n",
      "evaluate 0.4.6\n",
      "accelerate 1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Upgrade huggingface libs to versions that support evaluation_strategy, save_strategy, etc.\n",
    "!pip install -q --upgrade transformers accelerate datasets evaluate\n",
    "# show installed versions for quick sanity check\n",
    "import pkgutil, importlib\n",
    "import transformers, datasets, evaluate, accelerate\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"datasets\", datasets.__version__)\n",
    "print(\"evaluate\", evaluate.__version__)\n",
    "print(\"accelerate\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRq9LhvePuJ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "executionInfo": {
     "elapsed": 343680,
     "status": "ok",
     "timestamp": 1761813660651,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "WHVw5HDwOmQO",
    "outputId": "39a6e6c9-3daa-41c6-ee11-79a28c1455f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-444458563.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 05:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2552184760570526, 'eval_precision': 0.8099337748344371, 'eval_recall': 0.8463667820069204, 'eval_f1': 0.8277495769881558, 'eval_accuracy': 0.9321244792942907, 'eval_runtime': 7.3659, 'eval_samples_per_second': 135.761, 'eval_steps_per_second': 8.553, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(5000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ✅ Train and evaluate\n",
    "trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(eval_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2248,
     "status": "ok",
     "timestamp": 1761813754969,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "azP2VNeHSFEr",
    "outputId": "25e4f2bf-1c48-42af-edd5-c40f8c0bbb1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ner_model/tokenizer_config.json',\n",
       " './ner_model/special_tokens_map.json',\n",
       " './ner_model/vocab.txt',\n",
       " './ner_model/added_tokens.json',\n",
       " './ner_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Explicitly save full model + tokenizer with configuration\n",
    "model.save_pretrained(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1761813765583,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "e5jxKHe0PZmv",
    "outputId": "297eef61-49e8-44c7-e59d-b0685b857867"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Input: Barack Obama was born in Hawaii.\n",
      "  - Barack → LABEL_1 (score: 0.96)\n",
      "  - Obama → LABEL_2 (score: 0.98)\n",
      "  - was born in → LABEL_0 (score: 1.00)\n",
      "  - Hawaii → LABEL_5 (score: 0.97)\n",
      "  - . → LABEL_0 (score: 1.00)\n",
      "\n",
      "🔹 Input: Apple Inc. is headquartered in Cupertino, California.\n",
      "  - Apple → LABEL_3 (score: 0.88)\n",
      "  - Inc. → LABEL_4 (score: 0.94)\n",
      "  - is headquartered in → LABEL_0 (score: 1.00)\n",
      "  - Cup → LABEL_5 (score: 0.97)\n",
      "  - ##ertino, California → LABEL_6 (score: 0.90)\n",
      "  - . → LABEL_0 (score: 1.00)\n",
      "\n",
      "🔹 Input: Sachin Tendulkar played for Mumbai Indians in the IPL.\n",
      "  - Sachin → LABEL_1 (score: 0.80)\n",
      "  - Tendulkar → LABEL_2 (score: 0.96)\n",
      "  - played for → LABEL_0 (score: 1.00)\n",
      "  - Mumbai → LABEL_3 (score: 0.98)\n",
      "  - Indians → LABEL_4 (score: 0.98)\n",
      "  - in the → LABEL_0 (score: 1.00)\n",
      "  - IP → LABEL_3 (score: 0.90)\n",
      "  - ##L → LABEL_4 (score: 0.94)\n",
      "  - . → LABEL_0 (score: 1.00)\n",
      "\n",
      "🔹 Input: OpenAI developed ChatGPT for natural language processing.\n",
      "  - Open → LABEL_3 (score: 0.78)\n",
      "  - ##AI → LABEL_4 (score: 0.78)\n",
      "  - developed → LABEL_0 (score: 1.00)\n",
      "  - Cha → LABEL_3 (score: 0.84)\n",
      "  - ##tGPT → LABEL_4 (score: 0.83)\n",
      "  - for → LABEL_0 (score: 1.00)\n",
      "  - natural → LABEL_3 (score: 0.65)\n",
      "  - language → LABEL_4 (score: 0.92)\n",
      "  - processing. → LABEL_0 (score: 0.89)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model directly into a NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"./ner_model\",     # path to your trained model\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # groups subword tokens together\n",
    ")\n",
    "\n",
    "# Test sentences\n",
    "texts = [\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"Apple Inc. is headquartered in Cupertino, California.\",\n",
    "    \"Sachin Tendulkar played for Mumbai Indians in the IPL.\",\n",
    "    \"OpenAI developed ChatGPT for natural language processing.\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "for text in texts:\n",
    "    print(f\"\\n🔹 Input: {text}\")\n",
    "    results = ner_pipeline(text)\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']} → {entity['entity_group']} (score: {entity['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1761813817315,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "iWheG917Or2F",
    "outputId": "0ca1b6fd-9423-4bc6-838a-23499d94e83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Label → ID mapping:\n",
      "  LABEL_0         → 0\n",
      "  LABEL_1         → 1\n",
      "  LABEL_2         → 2\n",
      "  LABEL_3         → 3\n",
      "  LABEL_4         → 4\n",
      "  LABEL_5         → 5\n",
      "  LABEL_6         → 6\n",
      "\n",
      "🔹 ID → Label mapping:\n",
      "   0 → LABEL_0\n",
      "   1 → LABEL_1\n",
      "   2 → LABEL_2\n",
      "   3 → LABEL_3\n",
      "   4 → LABEL_4\n",
      "   5 → LABEL_5\n",
      "   6 → LABEL_6\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./ner_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./ner_model\")\n",
    "\n",
    "# View label mappings\n",
    "label2id = model.config.label2id\n",
    "id2label = model.config.id2label\n",
    "\n",
    "print(\"🔹 Label → ID mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label:15} → {idx}\")\n",
    "\n",
    "print(\"\\n🔹 ID → Label mapping:\")\n",
    "for idx, label in id2label.items():\n",
    "    print(f\"  {idx:2} → {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2623,
     "status": "ok",
     "timestamp": 1761813927090,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "Rli1pmblSWbT",
    "outputId": "e75d6f98-1315-42e5-ec73-c2dcf44254c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labels updated successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Path to your trained model folder\n",
    "model_dir = \"./ner_model\"\n",
    "\n",
    "# Use these labels (you can adjust if using a different dataset)\n",
    "labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "# Create label mappings\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Load your trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Attach label info\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Save back the updated config\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "print(\"✅ Labels updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1761813934802,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "mvDNJ5_eSwnZ",
    "outputId": "ddfa2831-5fb9-4382-dd2d-291d6e809283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 ID → Label mapping:\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./ner_model\")\n",
    "print(\"🔹 ID → Label mapping:\")\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1761813945434,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "UUYXGASgSzE6",
    "outputId": "59ad5e15-ceb0-4ffe-92bd-b9cf6c07f7f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Input: Barack Obama was born in Hawaii.\n",
      "  - Barack Obama → PER (score: 0.97)\n",
      "  - Hawaii → LOC (score: 0.97)\n",
      "\n",
      "🔹 Input: Apple Inc. is headquartered in Cupertino, California.\n",
      "  - Apple Inc. → ORG (score: 0.92)\n",
      "  - Cupertino, California → LOC (score: 0.91)\n",
      "\n",
      "🔹 Input: Sachin Tendulkar played for Mumbai Indians in the IPL.\n",
      "  - Sa → PER (score: 0.85)\n",
      "  - ##chin Tendulkar → PER (score: 0.92)\n",
      "  - Mumbai Indians → ORG (score: 0.98)\n",
      "  - IPL → ORG (score: 0.92)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"./ner_model\",\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"Apple Inc. is headquartered in Cupertino, California.\",\n",
    "    \"Sachin Tendulkar played for Mumbai Indians in the IPL.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"\\n🔹 Input: {text}\")\n",
    "    results = ner_pipeline(text)\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']} → {entity['entity_group']} (score: {entity['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1761814105260,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "UZ3dnCSjS1ov",
    "outputId": "f149a663-b6c3-4c26-8e51-ad5c149169ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content:\n",
      "conll2003_test.json   conll2003_validation.json  ner_model\n",
      "conll2003_train.json  logs\t\t\t sample_data\n",
      "\n",
      "/content/logs:\n",
      "events.out.tfevents.1761813177.6c14dae8594f.4484.0\n",
      "events.out.tfevents.1761813258.6c14dae8594f.4484.1\n",
      "events.out.tfevents.1761813318.6c14dae8594f.4484.2\n",
      "events.out.tfevents.1761813661.6c14dae8594f.4484.3\n",
      "\n",
      "/content/ner_model:\n",
      "checkpoint-500\tconfig.json\t   special_tokens_map.json  tokenizer.json\n",
      "checkpoint-939\tmodel.safetensors  tokenizer_config.json    vocab.txt\n",
      "\n",
      "/content/ner_model/checkpoint-500:\n",
      "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
      "model.safetensors  special_tokens_map.json  training_args.bin\n",
      "optimizer.pt\t   tokenizer_config.json    vocab.txt\n",
      "rng_state.pth\t   tokenizer.json\n",
      "\n",
      "/content/ner_model/checkpoint-939:\n",
      "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
      "model.safetensors  special_tokens_map.json  training_args.bin\n",
      "optimizer.pt\t   tokenizer_config.json    vocab.txt\n",
      "rng_state.pth\t   tokenizer.json\n",
      "\n",
      "/content/sample_data:\n",
      "anscombe.json\t\t      mnist_test.csv\n",
      "california_housing_test.csv   mnist_train_small.csv\n",
      "california_housing_train.csv  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls -R /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1761814227664,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "5RDrQlmVTcuD"
   },
   "outputs": [],
   "source": [
    "!rm /content/conll2003_*.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1761814239025,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "Rvb32JC4T6nH",
    "outputId": "4fcf5fc9-00de-403b-98f7-3dd325706738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/conll2003_*.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm /content/conll2003_*.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1761814250321,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "qO15rvRhT9Yo",
    "outputId": "e0746734-3e12-46dd-ddce-d788c819da2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content:\n",
      "logs  ner_model  sample_data\n",
      "\n",
      "/content/logs:\n",
      "events.out.tfevents.1761813177.6c14dae8594f.4484.0\n",
      "events.out.tfevents.1761813258.6c14dae8594f.4484.1\n",
      "events.out.tfevents.1761813318.6c14dae8594f.4484.2\n",
      "events.out.tfevents.1761813661.6c14dae8594f.4484.3\n",
      "\n",
      "/content/ner_model:\n",
      "checkpoint-500\tconfig.json\t   special_tokens_map.json  tokenizer.json\n",
      "checkpoint-939\tmodel.safetensors  tokenizer_config.json    vocab.txt\n",
      "\n",
      "/content/ner_model/checkpoint-500:\n",
      "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
      "model.safetensors  special_tokens_map.json  training_args.bin\n",
      "optimizer.pt\t   tokenizer_config.json    vocab.txt\n",
      "rng_state.pth\t   tokenizer.json\n",
      "\n",
      "/content/ner_model/checkpoint-939:\n",
      "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
      "model.safetensors  special_tokens_map.json  training_args.bin\n",
      "optimizer.pt\t   tokenizer_config.json    vocab.txt\n",
      "rng_state.pth\t   tokenizer.json\n",
      "\n",
      "/content/sample_data:\n",
      "anscombe.json\t\t      mnist_test.csv\n",
      "california_housing_test.csv   mnist_train_small.csv\n",
      "california_housing_train.csv  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls -R /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95018,
     "status": "ok",
     "timestamp": 1761814369652,
     "user": {
      "displayName": "22bd1a056g",
      "userId": "12822488165018302407"
     },
     "user_tz": -330
    },
    "id": "kZ1EZZIUUAJO",
    "outputId": "e5c3b6a9-084e-4fff-b178-00c03c4e6b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy the model and logs to Drive\n",
    "!cp -r /content/ner_model /content/drive/MyDrive/\n",
    "!cp -r /content/logs /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3dd0XKoUGG0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMk+RdqCuxGLhkKDLnJ5id4",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
